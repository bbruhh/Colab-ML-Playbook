{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building an Image Caption Generator.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Uo-ehm8geSRx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"60px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"20px\" vspace=\"5px\">\n",
        "\n",
        "# Building an Image Caption Generator"
      ]
    },
    {
      "metadata": {
        "id": "1Vvfx9mCevCf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* References:\n",
        "\n",
        "[https://medium.freecodecamp.org/building-an-image-caption-generator-with-deep-learning-in-tensorflow-a142722e9b1f](https://medium.freecodecamp.org/building-an-image-caption-generator-with-deep-learning-in-tensorflow-a142722e9b1f)"
      ]
    },
    {
      "metadata": {
        "id": "TaASyCVleh_C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/Image%20Caption%20Generator/Building%20an%20Image%20Caption%20Generator.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/Image%20Caption%20Generator/Building%20an%20Image%20Caption%20Generator.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "rqNU4oJKR7l4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "957b9e7e-76da-4925-eed6-65c62826bdd7"
      },
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "42WF7IzYeSTR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mapxCPxUeSTa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6333e283-c9ac-4101-df0c-369b96ae11ad"
      },
      "cell_type": "code",
      "source": [
        "## download_model.py\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import requests\n",
        "\n",
        "model_dict = {\n",
        "    'show-and-tell-2M': '15Juh0gaYR0qv8GjRL1EvsigErdQXTmnt'\n",
        "}\n",
        "\n",
        "\n",
        "def download_and_extract_model(model_name, data_dir):\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "\n",
        "    file_id = model_dict[model_name]\n",
        "    destination = os.path.join(data_dir, model_name + '.zip')\n",
        "    if not os.path.exists(destination):\n",
        "        print('Downloading model to %s' % destination)\n",
        "        download_file_from_google_drive(file_id, destination)\n",
        "        with zipfile.ZipFile(destination, 'r') as zip_ref:\n",
        "            print('Extracting model to %s' % data_dir)\n",
        "            zip_ref.extractall(data_dir)\n",
        "\n",
        "\n",
        "def download_file_from_google_drive(file_id, destination):\n",
        "    URL = \"https://drive.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params={'id': file_id}, stream=True)\n",
        "    token = get_confirm_token(response)\n",
        "    if token:\n",
        "        params = {'id': file_id, 'confirm': token}\n",
        "        response = session.get(URL, params=params, stream=True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:  # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "\n",
        "download_and_extract_model('show-and-tell-2M', 'model')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading model to model/show-and-tell-2M.zip\n",
            "Extracting model to model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "btKBBDFchDdS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00da6f60-0989-48a3-a1a1-93aeb73c4e6b"
      },
      "cell_type": "code",
      "source": [
        "!dir model"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "show-and-tell-2M.zip  show-and-tell.pb\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZFzpdv2QhVMF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## model.py\n",
        "\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class ShowAndTellModel(object):\n",
        "    def __init__(self, model_path):\n",
        "        self._model_path = model_path\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        self._load_model(model_path)\n",
        "        self._sess = tf.Session(graph=tf.get_default_graph())\n",
        "\n",
        "    def _load_model(self, frozen_graph_path):\n",
        "        \"\"\"\n",
        "        Loads a frozen graph\n",
        "        :param frozen_graph_path: path to .pb graph\n",
        "        :type frozen_graph_path: str\n",
        "        \"\"\"\n",
        "\n",
        "        model_exp = os.path.expanduser(frozen_graph_path)\n",
        "        if os.path.isfile(model_exp):\n",
        "            self.logger.info('Loading model filename: %s' % model_exp)\n",
        "            with tf.gfile.FastGFile(model_exp, 'rb') as f:\n",
        "                graph_def = tf.GraphDef()\n",
        "                graph_def.ParseFromString(f.read())\n",
        "                tf.import_graph_def(graph_def, name='')\n",
        "        else:\n",
        "            raise RuntimeError(\"Missing model file at path: {}\".format(frozen_graph_path))\n",
        "\n",
        "    def feed_image(self, encoded_image):\n",
        "        initial_state = self._sess.run(fetches=\"lstm/initial_state:0\",\n",
        "                                       feed_dict={\"image_feed:0\": encoded_image})\n",
        "        return initial_state\n",
        "\n",
        "    def inference_step(self, input_feed, state_feed):\n",
        "        softmax_output, state_output = self._sess.run(\n",
        "            fetches=[\"softmax:0\", \"lstm/state:0\"],\n",
        "            feed_dict={\n",
        "                \"input_feed:0\": input_feed,\n",
        "                \"lstm/state_feed:0\": state_feed,\n",
        "            })\n",
        "        return softmax_output, state_output, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DdwWHez8haJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f165ac5d-894d-4d2b-e8ed-681e938aa175"
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "# Download the file from 'url' and save it locally under `filename`:  \n",
        "def download(url):\n",
        "    filename = url.split('/')[-1]\n",
        "    print ('Downloading', filename)\n",
        "    f =  urllib.request.urlopen(url)\n",
        "    data = f.read()\n",
        "    f.close()\n",
        "    with open(filename, 'wb') as myfile:\n",
        "        myfile.write(data)    \n",
        "\n",
        "# download vocab file\n",
        "download('https://raw.githubusercontent.com/learning-stack/Colab-ML-Playbook/master/Image%20Caption%20Generator/word_counts.txt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading word_counts.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DJa1CmTBloYD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## vocabulary.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import logging\n",
        "import os\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    \"\"\"Vocabulary class for mapping words to ids\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_file_path,\n",
        "                 start_token=\"<S>\",\n",
        "                 end_token=\"</S>\",\n",
        "                 unk_token=\"<UNK>\"):\n",
        "        \"\"\"Initializes the vocabulary.\n",
        "    \n",
        "        Args:\n",
        "          vocab_file_path: File containing the vocabulary, where the tokens are the first\n",
        "            whitespace-separated token on each line (other tokens are ignored) and\n",
        "            the token ids are the corresponding line numbers.\n",
        "          start_token: Special token denoting sequence start.\n",
        "          end_token: Special token denoting sequence end.\n",
        "          unk_token: Special token denoting unknown tokens.\n",
        "        \"\"\"\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        if not os.path.exists(vocab_file_path):\n",
        "            self.logger.exception(\"Vocab file %s not found.\", vocab_file_path)\n",
        "            raise RuntimeError\n",
        "        self.logger.info(\"Initializing vocabulary from file: %s\", vocab_file_path)\n",
        "\n",
        "        with open(vocab_file_path, mode=\"r\") as f:\n",
        "            reverse_vocab = list(f.readlines())\n",
        "        reverse_vocab = [line.split()[0] for line in reverse_vocab]\n",
        "        assert start_token in reverse_vocab\n",
        "        assert end_token in reverse_vocab\n",
        "        if unk_token not in reverse_vocab:\n",
        "            reverse_vocab.append(unk_token)\n",
        "        vocab = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])\n",
        "\n",
        "        self.logger.info(\"Created vocabulary with %d words\" % len(vocab))\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.reverse_vocab = reverse_vocab\n",
        "\n",
        "        self.start_id = vocab[start_token]\n",
        "        self.end_id = vocab[end_token]\n",
        "        self.unk_id = vocab[unk_token]\n",
        "\n",
        "    def token_to_id(self, token_id):\n",
        "        if token_id in self.vocab:\n",
        "            return self.vocab[token_id]\n",
        "        else:\n",
        "            return self.unk_id\n",
        "\n",
        "    def id_to_token(self, token_id):\n",
        "        if token_id >= len(self.reverse_vocab):\n",
        "            return self.reverse_vocab[self.unk_id]\n",
        "        else:\n",
        "            return self.reverse_vocab[token_id]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "chouJ24JlX2B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## caption_generator.py\n",
        "\n",
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Class for generating captions from an image-to-text model.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import heapq\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TopN(object):\n",
        "    \"\"\"Maintains the top n elements of an incrementally provided set.\"\"\"\n",
        "\n",
        "    def __init__(self, n):\n",
        "        self._n = n\n",
        "        self._data = []\n",
        "\n",
        "    def size(self):\n",
        "        assert self._data is not None\n",
        "        return len(self._data)\n",
        "\n",
        "    def push(self, x):\n",
        "        \"\"\"Pushes a new element.\"\"\"\n",
        "        assert self._data is not None\n",
        "        if len(self._data) < self._n:\n",
        "            heapq.heappush(self._data, x)\n",
        "        else:\n",
        "            heapq.heappushpop(self._data, x)\n",
        "\n",
        "    def extract(self, sort=False):\n",
        "        \"\"\"Extracts all elements from the TopN. This is a destructive operation.\n",
        "    \n",
        "        The only method that can be called immediately after extract() is reset().\n",
        "    \n",
        "        Args:\n",
        "          sort: Whether to return the elements in descending sorted order.\n",
        "    \n",
        "        Returns:\n",
        "          A list of data; the top n elements provided to the set.\n",
        "        \"\"\"\n",
        "        assert self._data is not None\n",
        "        data = self._data\n",
        "        self._data = None\n",
        "        if sort:\n",
        "            data.sort(reverse=True)\n",
        "        return data\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Returns the TopN to an empty state.\"\"\"\n",
        "        self._data = []\n",
        "\n",
        "\n",
        "class Caption(object):\n",
        "    \"\"\"Represents a complete or partial caption.\"\"\"\n",
        "\n",
        "    def __init__(self, sentence, state, logprob, score, metadata=None):\n",
        "        \"\"\"Initializes the Caption.\n",
        "        Args:\n",
        "          sentence: List of word ids in the caption.\n",
        "          state: Model state after generating the previous word.\n",
        "          logprob: Log-probability of the caption.\n",
        "          score: Score of the caption.\n",
        "          metadata: Optional metadata associated with the partial sentence. If not\n",
        "            None, a list of strings with the same length as 'sentence'.\n",
        "        \"\"\"\n",
        "        self.sentence = sentence\n",
        "        self.state = state\n",
        "        self.logprob = logprob\n",
        "        self.score = score\n",
        "        self.metadata = metadata\n",
        "\n",
        "    def __cmp__(self, other):\n",
        "        \"\"\"Compares Captions by score.\"\"\"\n",
        "        assert isinstance(other, Caption)\n",
        "        if self.score == other.score:\n",
        "            return 0\n",
        "        elif self.score < other.score:\n",
        "            return -1\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    # For Python 3 compatibility (__cmp__ is deprecated).\n",
        "    def __lt__(self, other):\n",
        "        assert isinstance(other, Caption)\n",
        "        return self.score < other.score\n",
        "\n",
        "    # Also for Python 3 compatibility.\n",
        "    def __eq__(self, other):\n",
        "        assert isinstance(other, Caption)\n",
        "        return self.score == other.score\n",
        "\n",
        "\n",
        "class CaptionGenerator(object):\n",
        "    \"\"\"Class to generate captions from an image-to-text model.\n",
        "    This code is a modification of https://github.com/tensorflow/models/blob/master/research/im2txt/im2txt/inference_utils/caption_generator.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 vocab,\n",
        "                 beam_size=3,\n",
        "                 max_caption_length=20,\n",
        "                 length_normalization_factor=0.0):\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.model = model\n",
        "\n",
        "        self.beam_size = beam_size\n",
        "        self.max_caption_length = max_caption_length\n",
        "        self.length_normalization_factor = length_normalization_factor\n",
        "\n",
        "    def beam_search(self, encoded_image):\n",
        "        # Feed in the image to get the initial state.\n",
        "        partial_caption_beam = TopN(self.beam_size)\n",
        "        complete_captions = TopN(self.beam_size)\n",
        "        initial_state = self.model.feed_image(encoded_image)\n",
        "\n",
        "        initial_beam = Caption(\n",
        "            sentence=[self.vocab.start_id],\n",
        "            state=initial_state[0],\n",
        "            logprob=0.0,\n",
        "            score=0.0,\n",
        "            metadata=[\"\"])\n",
        "\n",
        "        partial_caption_beam.push(initial_beam)\n",
        "\n",
        "        # Run beam search.\n",
        "        for _ in range(self.max_caption_length - 1):\n",
        "            partial_captions_list = partial_caption_beam.extract()\n",
        "            partial_caption_beam.reset()\n",
        "            input_feed = np.array([c.sentence[-1] for c in partial_captions_list])\n",
        "            state_feed = np.array([c.state for c in partial_captions_list])\n",
        "\n",
        "            softmax, new_states, metadata = self.model.inference_step(input_feed,\n",
        "                                                                      state_feed)\n",
        "\n",
        "            for i, partial_caption in enumerate(partial_captions_list):\n",
        "                word_probabilities = softmax[i]\n",
        "                state = new_states[i]\n",
        "                # For this partial caption, get the beam_size most probable next words.\n",
        "                words_and_probs = list(enumerate(word_probabilities))\n",
        "                words_and_probs.sort(key=lambda x: -x[1])\n",
        "                words_and_probs = words_and_probs[0:self.beam_size]\n",
        "                # Each next word gives a new partial caption.\n",
        "                for w, p in words_and_probs:\n",
        "                    if p < 1e-12:\n",
        "                        continue  # Avoid log(0).\n",
        "                    sentence = partial_caption.sentence + [w]\n",
        "                    logprob = partial_caption.logprob + math.log(p)\n",
        "                    score = logprob\n",
        "                    if metadata:\n",
        "                        metadata_list = partial_caption.metadata + [metadata[i]]\n",
        "                    else:\n",
        "                        metadata_list = None\n",
        "                    if w == self.vocab.end_id:\n",
        "                        if self.length_normalization_factor > 0:\n",
        "                            score /= len(sentence) ** self.length_normalization_factor\n",
        "                        beam = Caption(sentence, state, logprob, score, metadata_list)\n",
        "                        complete_captions.push(beam)\n",
        "                    else:\n",
        "                        beam = Caption(sentence, state, logprob, score, metadata_list)\n",
        "                        partial_caption_beam.push(beam)\n",
        "            if partial_caption_beam.size() == 0:\n",
        "                # We have run out of partial candidates; happens when beam_size = 1.\n",
        "                break\n",
        "\n",
        "        # If we have no complete captions then fall back to the partial captions.\n",
        "        # But never output a mixture of complete and partial captions because a\n",
        "        # partial caption could have a higher score than all the complete captions.\n",
        "        if complete_captions.size() == 0:\n",
        "            complete_captions = partial_caption_beam\n",
        "\n",
        "        return complete_captions.extract(sort=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Rb1FoLoYPe7",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "48d599b3-6f1a-402a-8c2f-20e79d5a81c2"
      },
      "cell_type": "code",
      "source": [
        "# Download input file\n",
        "# download('https://github.com/learning-stack/Colab-ML-Playbook/raw/master/Image%20Caption%20Generator/trading_floor.jpg')\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ae6d732f-722c-4c59-80a8-142809f7a4d2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ae6d732f-722c-4c59-80a8-142809f7a4d2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving trading_floor.jpg to trading_floor.jpg\n",
            "User uploaded file \"trading_floor.jpg\" with length 214777 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AgAMyLBdl0uk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## inference.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "\n",
        "tf.flags.DEFINE_string(\"model_path\", \"model/show-and-tell.pb\", \"Model graph def path\")\n",
        "tf.flags.DEFINE_string(\"vocab_file\", \"word_counts.txt\", \"Text file containing the vocabulary.\")\n",
        "tf.flags.DEFINE_string(\"input_files\", \"sample.jpg\",\n",
        "                       \"File pattern or comma-separated list of file patterns \"\n",
        "                       \"of image files.\")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xB0t688MmM9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3e48370a-5751-4f60-e437-24b8f5e9ec95"
      },
      "cell_type": "code",
      "source": [
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "print(FLAGS.model_path)\n",
        "print(FLAGS.vocab_file)\n",
        "print(FLAGS.input_files)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model/show-and-tell.pb\n",
            "word_counts.txt\n",
            "trading_floor.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wY6ZW71KUEUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6409ea3-79ab-41c5-dbb6-13ac6c2e4aea"
      },
      "cell_type": "code",
      "source": [
        "model = ShowAndTellModel(FLAGS.model_path)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Loading model filename: model/show-and-tell.pb\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "tNG-JAI8ZkVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b15365a0-47d5-41c2-d068-95b3ed5ec6aa"
      },
      "cell_type": "code",
      "source": [
        "vocab = Vocabulary(FLAGS.vocab_file)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Initializing vocabulary from file: word_counts.txt\n",
            "INFO:__main__:Created vocabulary with 11519 words\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YXclhimu4ruf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3a32559-2694-47e0-9af6-faffa274846c"
      },
      "cell_type": "code",
      "source": [
        "def _load_filenames():\n",
        "    filenames = []\n",
        "    for file_pattern in FLAGS.input_files.split(\",\"):\n",
        "        filenames.extend(tf.gfile.Glob(file_pattern))\n",
        "    logger.info(\"Running caption generation on %d files matching %s\",\n",
        "                len(filenames), FLAGS.input_files)\n",
        "    return filenames\n",
        "\n",
        "filenames = _load_filenames()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Running caption generation on 1 files matching trading_floor.jpg\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "jjRM3IBiZncd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = CaptionGenerator(model, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXxkbQLH469m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1216f919-414d-4e3f-c7f2-2a629c7b5e65"
      },
      "cell_type": "code",
      "source": [
        "for filename in filenames:\n",
        "    with tf.gfile.GFile(filename, \"rb\") as f:\n",
        "        image = f.read()\n",
        "    captions = generator.beam_search(image)\n",
        "    print(\"Captions for image %s:\" % os.path.basename(filename))\n",
        "    for i, caption in enumerate(captions):\n",
        "        # Ignore begin and end tokens <S> and </S>.\n",
        "        sentence = [vocab.id_to_token(w) for w in caption.sentence[1:-1]]\n",
        "        sentence = \" \".join(sentence)\n",
        "        print(\"  %d) %s (p=%f)\" % (i, sentence, math.exp(caption.logprob)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Captions for image trading_floor.jpg:\n",
            "  0) a group of people sitting at tables in a room . (p=0.000306)\n",
            "  1) a group of people sitting around a table with laptops . (p=0.000140)\n",
            "  2) a group of people sitting at a table with laptops . (p=0.000069)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}