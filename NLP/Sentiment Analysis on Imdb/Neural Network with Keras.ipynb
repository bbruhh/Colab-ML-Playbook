{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network with Keras.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "L0dT79EZSDtB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"60px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"20px\" vspace=\"5px\">\n",
        "\n",
        "# Sentiment Analysis on Imdb Movie Reviews"
      ]
    },
    {
      "metadata": {
        "id": "RgW8s3sM7rDw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* References:\n",
        "\n",
        "[https://towardsdatascience.com/how-to-build-a-neural-network-with-keras-e8faa33d0ae4](https://towardsdatascience.com/how-to-build-a-neural-network-with-keras-e8faa33d0ae4)"
      ]
    },
    {
      "metadata": {
        "id": "j4gHxXEaSN4g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/NLP/Sentiment%20Analysis%20on%20Imdb/Neural%20Network%20with%20Keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/Sentiment%20Analysis%20on%20Imdb/Neural%20Network%20with%20Keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "M4b_JFtdsAoZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Import Dependencies and get the Data"
      ]
    },
    {
      "metadata": {
        "id": "pSgrLQiIS3UL",
        "colab_type": "code",
        "outputId": "60be6ac6-2939-447e-a5ee-e7538e586634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YjAkyzSyCxDm",
        "colab_type": "code",
        "outputId": "9c3ed84f-1e84-4495-9bc4-0645061c90f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#The imdb sentiment classification dataset consists of 50,000 movie reviews from imdb users that are labeled as either positive (1) or negative (0). \n",
        "#The reviews are preprocessed and each one is encoded as a sequence of word indexes in the form of integers. \n",
        "#The words within the reviews are indexed by their overall frequency within the dataset. For example, the integer “2” encodes the second most frequent word in the data. \n",
        "#The 50,000 reviews are split into 25,000 for training and 25,000 for testing. \n",
        "from keras.datasets import imdb\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "\n",
        "#We don’t want to have a 50/50 train test split, we merge the data into data and targets after downloading, so that we can do an 80/20 split later on.\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NMOzP9wEsJWh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Exploring the Data"
      ]
    },
    {
      "metadata": {
        "id": "VsluPna2Cypl",
        "colab_type": "code",
        "outputId": "db498fd0-2c30-4b8c-bc81-56e46e9f3b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Categories:\", np.unique(targets))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "\n",
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))\n",
        "print(\"Label:\", targets[0])\n",
        "\n",
        "#First review of the dataset which is labeled as positive (1). \n",
        "print(data[0])\n",
        "\n",
        "#The code below retrieves the dictionary mapping word indices back into the original words so that we can read them. \n",
        "#It replaces every unknown word with a “#”. It does this by using the get_word_index() function.\n",
        "index = imdb.get_word_index()\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] )\n",
        "print(decoded) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categories: [0 1]\n",
            "Number of unique words: 9998\n",
            "Average Review length: 234.75892\n",
            "Standard Deviation: 173.0\n",
            "Label: 1\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 2s 1us/step\n",
            "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TqeBS6mSsMQR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Data Preparation"
      ]
    },
    {
      "metadata": {
        "id": "JNMlD8egDEl1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We will vectorize every review and fill it with zeros so that it contains exactly 10,000 numbers. That means we fill every review that is shorter than 10,000 with zeros. \n",
        "#We do this because the biggest review is nearly that long and every input for our neural network needs to have the same size.\n",
        "#We also transform the targets into floats.\n",
        "def vectorize(sequences, dimension = 10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1\n",
        "  return results\n",
        " \n",
        "data = vectorize(data)\n",
        "targets = np.array(targets).astype(\"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jkRaEx4FDSoa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Now we split our data into a training and a testing set. The training set will contain 40,000 reviews and the testing set 10,000.\n",
        "test_x = data[:10000]\n",
        "test_y = targets[:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = targets[10000:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gx3CgBLDsPvJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Building and Training the Model"
      ]
    },
    {
      "metadata": {
        "id": "Sy_XHjajzQD5",
        "colab_type": "code",
        "outputId": "377f8361-e9fc-4503-ef19-7d2ff7a54335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "# Input - Layer\n",
        "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
        "# Hidden - Layers\n",
        "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "# Output- Layer\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 50)                500050    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 505,201\n",
            "Trainable params: 505,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qHxe2GKOsTLh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## compiling the model"
      ]
    },
    {
      "metadata": {
        "id": "tOC2Y4_2FdKg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We use the “adam” optimizer. The optimizer is the algorithm that changes the weights and biases during training. \n",
        "#We also choose binary-crossentropy as loss (because we deal with binary classification) and accuracy as our evaluation metric.\n",
        "model.compile(\n",
        " optimizer = \"adam\",\n",
        " loss = \"binary_crossentropy\",\n",
        " metrics = [\"accuracy\"]\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KACt9_gCsVyZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Train our model"
      ]
    },
    {
      "metadata": {
        "id": "KviqCj5KVyKR",
        "colab_type": "code",
        "outputId": "7f1f6894-e785-4817-8817-e9f1953949e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#We do this with a batch_size of 500 and only for two epochs because I recognized that the model overfits if we train it longer. \n",
        "#The Batch size defines the number of samples that will be propagated through the network and an epoch is an iteration over the entire training data. \n",
        "#In general a larger batch-size results in faster training, but don’t always converges fast. \n",
        "#A smaller batch-size is slower in training but it can converge faster. This is definitely problem dependent and you need to try out a few different values. \n",
        "#If you start with a problem for the first time, I would you recommend to you to first use a batch-size of 32, which is the standard size.\n",
        "results = model.fit(\n",
        " train_x, train_y,\n",
        " epochs= 2,\n",
        " batch_size = 500,\n",
        " validation_data = (test_x, test_y)\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 5s 125us/step - loss: 0.4061 - acc: 0.8198 - val_loss: 0.2641 - val_acc: 0.8935\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 3s 86us/step - loss: 0.2120 - acc: 0.9203 - val_loss: 0.2608 - val_acc: 0.8955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "541oUhVIsYvR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Evaluate our model"
      ]
    },
    {
      "metadata": {
        "id": "pGEasn5XaGrH",
        "colab_type": "code",
        "outputId": "583b8f93-d8d0-4332-9132-17f4f1ee68e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Test-Accuracy:\", np.mean(results.history[\"val_acc\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test-Accuracy: 0.8945000082254411\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}