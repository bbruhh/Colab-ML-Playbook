{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie-descriptions-tfhub.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "feFbqZpP1soY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"60px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"20px\" vspace=\"5px\">\n",
        "\n",
        "# Building a text classification model with TF Hub\n",
        "\n",
        "In this notebook, we'll walk you through building a model to predict the genres of a movie given its description. The emphasis here is not on accuracy, but instead how to use TF Hub layers in a text classification model.\n",
        "\n",
        "To start, import the necessary dependencies for this project."
      ]
    },
    {
      "metadata": {
        "id": "htnOxl3IAB_O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* References:\n",
        "\n",
        "[https://medium.com/tensorflow/building-a-text-classification-model-with-tensorflow-hub-and-estimators-3169e7aa568](https://medium.com/tensorflow/building-a-text-classification-model-with-tensorflow-hub-and-estimators-3169e7aa568)"
      ]
    },
    {
      "metadata": {
        "id": "dCc2rvJsAHTz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/NLP/NLP%20-%20Text%20Classification%20Model%20With%20Tensorflow%20Hub/movie_descriptions_tfhub.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/NLP/NLP%20-%20Text%20Classification%20Model%20With%20Tensorflow%20Hub/movie_descriptions_tfhub.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "rOEllRxGQ_me",
        "colab_type": "code",
        "outputId": "3067c358-ffcd-4b43-cb52-e93b8f8f9bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import json\n",
        "import pickle\n",
        "import urllib\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xvwN2Jkx2CdU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The dataset\n",
        "\n",
        "We need a lot of text inputs to train our model. For this model we'll use [this awesome movies dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset) from Kaggle. To simplify things I've made the `movies_metadata.csv` file available in a public Cloud Storage bucket so we can download it with `wget`. I've preprocessed the dataset already to limit the number of genres we'll use for our model, but first let's take a look at the original data so we can see what we're working with."
      ]
    },
    {
      "metadata": {
        "id": "YObfZBenyfMT",
        "colab_type": "code",
        "outputId": "0db546cd-0a21-4411-9b3f-1d04b31f8da3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Download the data from GCS\n",
        "!wget 'https://storage.googleapis.com/movies_data/movies_metadata.csv'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log’.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WFKB0Bw62xW-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we'll convert the dataset to a Pandas dataframe and print the first 5 rows. For this model we're only using 2 of these columns: `genres` and `overview`."
      ]
    },
    {
      "metadata": {
        "id": "NaZiKWtGyoQE",
        "colab_type": "code",
        "outputId": "87248332-63d3-4d1c-ac98-18c46e9f3cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('movies_metadata.csv')\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>adult</th>\n",
              "      <th>belongs_to_collection</th>\n",
              "      <th>budget</th>\n",
              "      <th>genres</th>\n",
              "      <th>homepage</th>\n",
              "      <th>id</th>\n",
              "      <th>imdb_id</th>\n",
              "      <th>original_language</th>\n",
              "      <th>original_title</th>\n",
              "      <th>overview</th>\n",
              "      <th>...</th>\n",
              "      <th>release_date</th>\n",
              "      <th>revenue</th>\n",
              "      <th>runtime</th>\n",
              "      <th>spoken_languages</th>\n",
              "      <th>status</th>\n",
              "      <th>tagline</th>\n",
              "      <th>title</th>\n",
              "      <th>video</th>\n",
              "      <th>vote_average</th>\n",
              "      <th>vote_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
              "      <td>30000000</td>\n",
              "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
              "      <td>http://toystory.disney.com/toy-story</td>\n",
              "      <td>862</td>\n",
              "      <td>tt0114709</td>\n",
              "      <td>en</td>\n",
              "      <td>Toy Story</td>\n",
              "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
              "      <td>...</td>\n",
              "      <td>1995-10-30</td>\n",
              "      <td>373554033.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
              "      <td>Released</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Toy Story</td>\n",
              "      <td>False</td>\n",
              "      <td>7.7</td>\n",
              "      <td>5415.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>65000000</td>\n",
              "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8844</td>\n",
              "      <td>tt0113497</td>\n",
              "      <td>en</td>\n",
              "      <td>Jumanji</td>\n",
              "      <td>When siblings Judy and Peter discover an encha...</td>\n",
              "      <td>...</td>\n",
              "      <td>1995-12-15</td>\n",
              "      <td>262797249.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
              "      <td>Released</td>\n",
              "      <td>Roll the dice and unleash the excitement!</td>\n",
              "      <td>Jumanji</td>\n",
              "      <td>False</td>\n",
              "      <td>6.9</td>\n",
              "      <td>2413.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>{'id': 119050, 'name': 'Grumpy Old Men Collect...</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15602</td>\n",
              "      <td>tt0113228</td>\n",
              "      <td>en</td>\n",
              "      <td>Grumpier Old Men</td>\n",
              "      <td>A family wedding reignites the ancient feud be...</td>\n",
              "      <td>...</td>\n",
              "      <td>1995-12-22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
              "      <td>Released</td>\n",
              "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
              "      <td>Grumpier Old Men</td>\n",
              "      <td>False</td>\n",
              "      <td>6.5</td>\n",
              "      <td>92.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16000000</td>\n",
              "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>31357</td>\n",
              "      <td>tt0114885</td>\n",
              "      <td>en</td>\n",
              "      <td>Waiting to Exhale</td>\n",
              "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
              "      <td>...</td>\n",
              "      <td>1995-12-22</td>\n",
              "      <td>81452156.0</td>\n",
              "      <td>127.0</td>\n",
              "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
              "      <td>Released</td>\n",
              "      <td>Friends are the people who let you be yourself...</td>\n",
              "      <td>Waiting to Exhale</td>\n",
              "      <td>False</td>\n",
              "      <td>6.1</td>\n",
              "      <td>34.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>{'id': 96871, 'name': 'Father of the Bride Col...</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11862</td>\n",
              "      <td>tt0113041</td>\n",
              "      <td>en</td>\n",
              "      <td>Father of the Bride Part II</td>\n",
              "      <td>Just when George Banks has recovered from his ...</td>\n",
              "      <td>...</td>\n",
              "      <td>1995-02-10</td>\n",
              "      <td>76578911.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
              "      <td>Released</td>\n",
              "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
              "      <td>Father of the Bride Part II</td>\n",
              "      <td>False</td>\n",
              "      <td>5.7</td>\n",
              "      <td>173.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   adult                              belongs_to_collection    budget  \\\n",
              "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
              "1  False                                                NaN  65000000   \n",
              "2  False  {'id': 119050, 'name': 'Grumpy Old Men Collect...         0   \n",
              "3  False                                                NaN  16000000   \n",
              "4  False  {'id': 96871, 'name': 'Father of the Bride Col...         0   \n",
              "\n",
              "                                              genres  \\\n",
              "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
              "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
              "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
              "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
              "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
              "\n",
              "                               homepage     id    imdb_id original_language  \\\n",
              "0  http://toystory.disney.com/toy-story    862  tt0114709                en   \n",
              "1                                   NaN   8844  tt0113497                en   \n",
              "2                                   NaN  15602  tt0113228                en   \n",
              "3                                   NaN  31357  tt0114885                en   \n",
              "4                                   NaN  11862  tt0113041                en   \n",
              "\n",
              "                original_title  \\\n",
              "0                    Toy Story   \n",
              "1                      Jumanji   \n",
              "2             Grumpier Old Men   \n",
              "3            Waiting to Exhale   \n",
              "4  Father of the Bride Part II   \n",
              "\n",
              "                                            overview    ...     release_date  \\\n",
              "0  Led by Woody, Andy's toys live happily in his ...    ...       1995-10-30   \n",
              "1  When siblings Judy and Peter discover an encha...    ...       1995-12-15   \n",
              "2  A family wedding reignites the ancient feud be...    ...       1995-12-22   \n",
              "3  Cheated on, mistreated and stepped on, the wom...    ...       1995-12-22   \n",
              "4  Just when George Banks has recovered from his ...    ...       1995-02-10   \n",
              "\n",
              "       revenue runtime                                   spoken_languages  \\\n",
              "0  373554033.0    81.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
              "1  262797249.0   104.0  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...   \n",
              "2          0.0   101.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
              "3   81452156.0   127.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
              "4   76578911.0   106.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
              "\n",
              "     status                                            tagline  \\\n",
              "0  Released                                                NaN   \n",
              "1  Released          Roll the dice and unleash the excitement!   \n",
              "2  Released  Still Yelling. Still Fighting. Still Ready for...   \n",
              "3  Released  Friends are the people who let you be yourself...   \n",
              "4  Released  Just When His World Is Back To Normal... He's ...   \n",
              "\n",
              "                         title  video vote_average vote_count  \n",
              "0                    Toy Story  False          7.7     5415.0  \n",
              "1                      Jumanji  False          6.9     2413.0  \n",
              "2             Grumpier Old Men  False          6.5       92.0  \n",
              "3            Waiting to Exhale  False          6.1       34.0  \n",
              "4  Father of the Bride Part II  False          5.7      173.0  \n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "MBLcNSE_7Icv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the data for our model\n",
        "\n",
        "I've done some preprocessing to limit the dataset to the top 9 genres, and I've saved the Pandas dataframes as public [Pickle](https://docs.python.org/3/library/pickle.html) files in GCS. Here we download those files. The resulting `descriptions` and `genres` variables are Pandas Series containing all descriptions and genres from our dataset respectively."
      ]
    },
    {
      "metadata": {
        "id": "rzjJuKhir-PH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve('https://storage.googleapis.com/bq-imports/descriptions.p', 'descriptions.p')\n",
        "urllib.request.urlretrieve('https://storage.googleapis.com/bq-imports/genres.p', 'genres.p')\n",
        "\n",
        "descriptions = pickle.load(open('descriptions.p', 'rb'))\n",
        "genres = pickle.load(open('genres.p', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lfb6TpuZAogw",
        "colab_type": "code",
        "outputId": "d5aeae36-7514-46d5-be4d-fc5d98d39bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        }
      },
      "cell_type": "code",
      "source": [
        "print (genres)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2                           [Romance, Comedy]\n",
            "4                                    [Comedy]\n",
            "6                           [Comedy, Romance]\n",
            "8               [Action, Adventure, Thriller]\n",
            "9               [Adventure, Action, Thriller]\n",
            "11                           [Comedy, Horror]\n",
            "14                        [Action, Adventure]\n",
            "17                            [Crime, Comedy]\n",
            "18                 [Crime, Comedy, Adventure]\n",
            "19                    [Action, Comedy, Crime]\n",
            "20                  [Comedy, Thriller, Crime]\n",
            "22       [Action, Adventure, Crime, Thriller]\n",
            "32                       [Romance, Adventure]\n",
            "51                          [Comedy, Romance]\n",
            "62                                   [Comedy]\n",
            "63                          [Comedy, Romance]\n",
            "64                                   [Comedy]\n",
            "65                  [Action, Science Fiction]\n",
            "67                          [Comedy, Romance]\n",
            "68                                   [Comedy]\n",
            "69          [Horror, Action, Thriller, Crime]\n",
            "70                [Action, Thriller, Romance]\n",
            "75                  [Horror, Science Fiction]\n",
            "76                              [Documentary]\n",
            "87                                   [Comedy]\n",
            "91                  [Comedy, Horror, Romance]\n",
            "94                                   [Comedy]\n",
            "97                              [Documentary]\n",
            "100                         [Comedy, Romance]\n",
            "102                                  [Comedy]\n",
            "                         ...                 \n",
            "45369                                [Comedy]\n",
            "45373                         [Comedy, Crime]\n",
            "45376                       [Comedy, Romance]\n",
            "45377                             [Adventure]\n",
            "45380                       [Romance, Comedy]\n",
            "45389                           [Documentary]\n",
            "45390                                [Horror]\n",
            "45391                               [Romance]\n",
            "45398                       [Romance, Comedy]\n",
            "45399                         [Crime, Comedy]\n",
            "45402                                [Comedy]\n",
            "45406                       [Comedy, Romance]\n",
            "45407                                [Comedy]\n",
            "45408                                [Comedy]\n",
            "45414                           [Documentary]\n",
            "45415                           [Documentary]\n",
            "45416                                [Comedy]\n",
            "45419                           [Documentary]\n",
            "45424                             [Adventure]\n",
            "45426                        [Comedy, Horror]\n",
            "45428             [Action, Romance, Thriller]\n",
            "45429          [Documentary, Science Fiction]\n",
            "45430             [Thriller, Science Fiction]\n",
            "45431                           [Documentary]\n",
            "45432                           [Documentary]\n",
            "45435                      [Thriller, Horror]\n",
            "45441                                [Comedy]\n",
            "45452                           [Documentary]\n",
            "45458                                [Horror]\n",
            "45459                       [Science Fiction]\n",
            "Name: genres, Length: 14993, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZUypuN818T_D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Splitting our data\n",
        "When we train our model, we'll use 80% of the data for training and set aside 20% of the data to evaluate how our model performed."
      ]
    },
    {
      "metadata": {
        "id": "_nticMcj1alW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_size = int(len(descriptions) * .8)\n",
        "\n",
        "train_descriptions = descriptions[:train_size].astype('str')\n",
        "train_genres = genres[:train_size]\n",
        "\n",
        "test_descriptions = descriptions[train_size:].astype('str')\n",
        "test_genres = genres[train_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FmZ9iqK88nSD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Formatting our labels\n",
        "When we train our model we'll provide the labels (in this case genres) associated with each movie. We can't pass the genres in as strings directly, we'll transform them into multi-hot vectors. Since we have 9 genres, we'll have a 9 element vector for each movie with 0s and 1s indicating which genres are present in each description."
      ]
    },
    {
      "metadata": {
        "id": "bouv0R-D7J45",
        "colab_type": "code",
        "outputId": "59fcd248-255d-4be7-ee4a-411572baae1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "encoder = MultiLabelBinarizer()\n",
        "encoder.fit_transform(train_genres)\n",
        "train_encoded = encoder.transform(train_genres)\n",
        "test_encoded = encoder.transform(test_genres)\n",
        "num_classes = len(encoder.classes_)\n",
        "\n",
        "# Print all possible genres and the labels for the first movie in our training dataset\n",
        "print(encoder.classes_)\n",
        "print(train_encoded[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Action' 'Adventure' 'Comedy' 'Crime' 'Documentary' 'Horror' 'Romance'\n",
            " 'Science Fiction' 'Thriller']\n",
            "[0 0 1 0 0 0 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ir8ez0K_9sYA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create our TF Hub embedding layer\n",
        "Our model will only have one feature (the description) and it’ll be represented as an embedding column. Text embeddings provide a way to represent pieces of text in vector space, so that similar words or sentences are closer together in the embedding space. TF Hub simplifies this process by providing text embeddings that have already been trained on a variety of text data.\n",
        "\n",
        "\n",
        "[TF Hub]() provides a library of existing pre-trained model checkpoints for various kinds of models (images, text, and more) In this model we'll use the TF Hub `universal-sentence-encoder` module for our pre-trained word embeddings. **Universal sentence encoder is famous for longer form text inputs**. We only need one line of code to instantiate module. When we train our model, it'll convert our array of movie description strings to embeddings. When we train our model, we'll use this as a feature column.\n"
      ]
    },
    {
      "metadata": {
        "id": "PWuNUXq7a-7p",
        "colab_type": "code",
        "outputId": "5b2469d8-311e-42a9-a73f-8c3aa83d72c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "description_embeddings = hub.text_embedding_column(\"descriptions\", module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/2\", trainable=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/2'.\n",
            "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/2'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9vscf4Fo-iI-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Instantiating our DNNEstimator Model\n",
        "\n",
        "Since each label needs to be the same length, we’ll transform these lists into multi-hot vectors of 1s and 0s corresponding to the genres present in a particular description.\n",
        "\n",
        "\n",
        "The first parameter we pass to our DNNEstimator is called a head, and defines the type of labels our model should expect. Since we want our model to output multiple labels, we’ll use multi_label_head here. Then we'll convert our features and labels to numpy arrays and instantiate our Estimator. `batch_size` and `num_epochs` are hyperparameters - you should experiment with different values to see what works best on your dataset.\n",
        "\n",
        "\n",
        "To transform our string labels into multi-hot vectors in just a few lines of code we’ll use a scikit learn utility called MultiLabelBinarizer."
      ]
    },
    {
      "metadata": {
        "id": "c0Vsmu9O21je",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multi_label_head = tf.contrib.estimator.multi_label_head(\n",
        "    num_classes,\n",
        "    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FLYoxsuNG4yH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The hidden_units param indicates how many layers we’ll have in our network. This model has 2 layers, the first has 64 neurons and the second has 10. "
      ]
    },
    {
      "metadata": {
        "id": "8mTpWD_Q8GKe",
        "colab_type": "code",
        "outputId": "040444d8-f9d4-4dae-9a2e-13e080b4dfe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "features = {\n",
        "  \"descriptions\": np.array(train_descriptions).astype(np.str)\n",
        "}\n",
        "labels = np.array(train_encoded).astype(np.int32)\n",
        "train_input_fn = tf.estimator.inputs.numpy_input_fn(features, labels, shuffle=True, batch_size=32, num_epochs=25)\n",
        "estimator = tf.contrib.estimator.DNNEstimator(\n",
        "    head=multi_label_head,\n",
        "    hidden_units=[64,10],\n",
        "    feature_columns=[description_embeddings])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp3ybvbz_d\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp3ybvbz_d', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6aac46d550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5ak1cZPZ_ZYM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and serving our model \n",
        "To train our model, we simply call `train()` passing it the input function we defined above. Once our model is trained, we'll define an evaluation input function similar to the one above and call `evaluate()`. When this completes we'll get a few metrics we can use to evaluate our model's accuracy.\n"
      ]
    },
    {
      "metadata": {
        "id": "jmtvJ5o3Olcg",
        "colab_type": "code",
        "outputId": "07ac446a-9b58-45a5-a9e1-a1dd636fb89a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3400
        }
      },
      "cell_type": "code",
      "source": [
        "estimator.train(input_fn=train_input_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp3ybvbz_d/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.693498, step = 1\n",
            "INFO:tensorflow:global_step/sec: 47.6126\n",
            "INFO:tensorflow:loss = 0.48309192, step = 101 (2.106 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.0742\n",
            "INFO:tensorflow:loss = 0.44566014, step = 201 (1.752 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.3147\n",
            "INFO:tensorflow:loss = 0.43010002, step = 301 (1.772 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.8207\n",
            "INFO:tensorflow:loss = 0.39764452, step = 401 (1.791 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.1101\n",
            "INFO:tensorflow:loss = 0.39083415, step = 501 (1.751 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.7123\n",
            "INFO:tensorflow:loss = 0.36367542, step = 601 (1.766 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.3128\n",
            "INFO:tensorflow:loss = 0.37100556, step = 701 (1.773 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.3427\n",
            "INFO:tensorflow:loss = 0.42141587, step = 801 (1.807 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.6171\n",
            "INFO:tensorflow:loss = 0.3280553, step = 901 (1.738 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.9336\n",
            "INFO:tensorflow:loss = 0.35169944, step = 1001 (1.758 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.7467\n",
            "INFO:tensorflow:loss = 0.3810224, step = 1101 (1.790 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.4535\n",
            "INFO:tensorflow:loss = 0.445812, step = 1201 (1.771 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.6605\n",
            "INFO:tensorflow:loss = 0.4067155, step = 1301 (1.768 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.0606\n",
            "INFO:tensorflow:loss = 0.31472227, step = 1401 (1.753 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.2024\n",
            "INFO:tensorflow:loss = 0.2828576, step = 1501 (1.781 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.9704\n",
            "INFO:tensorflow:loss = 0.38374922, step = 1601 (1.785 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.7689\n",
            "INFO:tensorflow:loss = 0.27484372, step = 1701 (1.763 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.8766\n",
            "INFO:tensorflow:loss = 0.30984604, step = 1801 (1.785 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.7541\n",
            "INFO:tensorflow:loss = 0.28778976, step = 1901 (1.794 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.8445\n",
            "INFO:tensorflow:loss = 0.38398355, step = 2001 (1.760 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.3748\n",
            "INFO:tensorflow:loss = 0.2767023, step = 2101 (1.746 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.1579\n",
            "INFO:tensorflow:loss = 0.30385047, step = 2201 (1.777 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.4609\n",
            "INFO:tensorflow:loss = 0.3362946, step = 2301 (1.806 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.7873\n",
            "INFO:tensorflow:loss = 0.32495025, step = 2401 (1.728 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.2194\n",
            "INFO:tensorflow:loss = 0.26070553, step = 2501 (1.779 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.3915\n",
            "INFO:tensorflow:loss = 0.24192262, step = 2601 (1.776 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.5769\n",
            "INFO:tensorflow:loss = 0.31683445, step = 2701 (1.765 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.2196\n",
            "INFO:tensorflow:loss = 0.27904156, step = 2801 (1.748 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.5996\n",
            "INFO:tensorflow:loss = 0.26039806, step = 2901 (1.767 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.2018\n",
            "INFO:tensorflow:loss = 0.2658054, step = 3001 (1.779 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.6279\n",
            "INFO:tensorflow:loss = 0.30757254, step = 3101 (1.766 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.0791\n",
            "INFO:tensorflow:loss = 0.24413739, step = 3201 (1.752 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.9398\n",
            "INFO:tensorflow:loss = 0.3046639, step = 3301 (1.788 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.7509\n",
            "INFO:tensorflow:loss = 0.29104373, step = 3401 (1.794 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.7741\n",
            "INFO:tensorflow:loss = 0.28290626, step = 3501 (1.735 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.1351\n",
            "INFO:tensorflow:loss = 0.2751609, step = 3601 (1.746 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.8233\n",
            "INFO:tensorflow:loss = 0.24395002, step = 3701 (1.760 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.583\n",
            "INFO:tensorflow:loss = 0.2778826, step = 3801 (1.803 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.876\n",
            "INFO:tensorflow:loss = 0.32951683, step = 3901 (1.724 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.9963\n",
            "INFO:tensorflow:loss = 0.3059104, step = 4001 (1.789 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.0178\n",
            "INFO:tensorflow:loss = 0.2757391, step = 4101 (1.785 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.9667\n",
            "INFO:tensorflow:loss = 0.32967156, step = 4201 (1.784 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.5385\n",
            "INFO:tensorflow:loss = 0.25475264, step = 4301 (1.738 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.4346\n",
            "INFO:tensorflow:loss = 0.23528984, step = 4401 (1.774 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.5734\n",
            "INFO:tensorflow:loss = 0.2026693, step = 4501 (1.765 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.4042\n",
            "INFO:tensorflow:loss = 0.28416184, step = 4601 (1.746 sec)\n",
            "INFO:tensorflow:global_step/sec: 53.8911\n",
            "INFO:tensorflow:loss = 0.23752706, step = 4701 (1.857 sec)\n",
            "INFO:tensorflow:global_step/sec: 52.9865\n",
            "INFO:tensorflow:loss = 0.26084968, step = 4801 (1.882 sec)\n",
            "INFO:tensorflow:global_step/sec: 52.2477\n",
            "INFO:tensorflow:loss = 0.21825308, step = 4901 (1.914 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.811\n",
            "INFO:tensorflow:loss = 0.3420145, step = 5001 (1.760 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.9049\n",
            "INFO:tensorflow:loss = 0.2245194, step = 5101 (1.758 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.0628\n",
            "INFO:tensorflow:loss = 0.23737933, step = 5201 (1.783 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.4254\n",
            "INFO:tensorflow:loss = 0.2633237, step = 5301 (1.808 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.9747\n",
            "INFO:tensorflow:loss = 0.29690337, step = 5401 (1.752 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.2056\n",
            "INFO:tensorflow:loss = 0.2635919, step = 5501 (1.779 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.5095\n",
            "INFO:tensorflow:loss = 0.26458648, step = 5601 (1.770 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.5875\n",
            "INFO:tensorflow:loss = 0.28637135, step = 5701 (1.799 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.6619\n",
            "INFO:tensorflow:loss = 0.23983413, step = 5801 (1.734 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.1883\n",
            "INFO:tensorflow:loss = 0.22989267, step = 5901 (1.780 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.256\n",
            "INFO:tensorflow:loss = 0.28031033, step = 6001 (1.777 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.7058\n",
            "INFO:tensorflow:loss = 0.22533202, step = 6101 (1.764 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.7039\n",
            "INFO:tensorflow:loss = 0.27068007, step = 6201 (1.764 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.8125\n",
            "INFO:tensorflow:loss = 0.3015894, step = 6301 (1.792 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.1325\n",
            "INFO:tensorflow:loss = 0.25081712, step = 6401 (1.781 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.3986\n",
            "INFO:tensorflow:loss = 0.3069029, step = 6501 (1.742 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.3331\n",
            "INFO:tensorflow:loss = 0.20466995, step = 6601 (1.744 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.6556\n",
            "INFO:tensorflow:loss = 0.249097, step = 6701 (1.797 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.955\n",
            "INFO:tensorflow:loss = 0.29454118, step = 6801 (1.791 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.7132\n",
            "INFO:tensorflow:loss = 0.23998712, step = 6901 (1.729 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.9574\n",
            "INFO:tensorflow:loss = 0.21456575, step = 7001 (1.790 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.4958\n",
            "INFO:tensorflow:loss = 0.21933115, step = 7101 (1.771 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.8581\n",
            "INFO:tensorflow:loss = 0.31606156, step = 7201 (1.786 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.6744\n",
            "INFO:tensorflow:loss = 0.2060861, step = 7301 (1.734 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.4061\n",
            "INFO:tensorflow:loss = 0.20749827, step = 7401 (1.777 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.7066\n",
            "INFO:tensorflow:loss = 0.28260916, step = 7501 (1.791 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.3268\n",
            "INFO:tensorflow:loss = 0.28032482, step = 7601 (1.775 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.845\n",
            "INFO:tensorflow:loss = 0.28055143, step = 7701 (1.759 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.1217\n",
            "INFO:tensorflow:loss = 0.2345896, step = 7801 (1.784 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.3164\n",
            "INFO:tensorflow:loss = 0.2684272, step = 7901 (1.773 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.4132\n",
            "INFO:tensorflow:loss = 0.2743984, step = 8001 (1.742 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.545\n",
            "INFO:tensorflow:loss = 0.2424701, step = 8101 (1.769 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.8128\n",
            "INFO:tensorflow:loss = 0.25009185, step = 8201 (1.763 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.0569\n",
            "INFO:tensorflow:loss = 0.2980088, step = 8301 (1.784 sec)\n",
            "INFO:tensorflow:global_step/sec: 58.6883\n",
            "INFO:tensorflow:loss = 0.28769547, step = 8401 (1.701 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.1351\n",
            "INFO:tensorflow:loss = 0.20764339, step = 8501 (1.784 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.7763\n",
            "INFO:tensorflow:loss = 0.25347397, step = 8601 (1.758 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.7283\n",
            "INFO:tensorflow:loss = 0.38800377, step = 8701 (1.794 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.871\n",
            "INFO:tensorflow:loss = 0.25911677, step = 8801 (1.728 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.6928\n",
            "INFO:tensorflow:loss = 0.25378582, step = 8901 (1.796 sec)\n",
            "INFO:tensorflow:global_step/sec: 56.3236\n",
            "INFO:tensorflow:loss = 0.21314819, step = 9001 (1.776 sec)\n",
            "INFO:tensorflow:global_step/sec: 55.4684\n",
            "INFO:tensorflow:loss = 0.26242518, step = 9101 (1.803 sec)\n",
            "INFO:tensorflow:global_step/sec: 52.0046\n",
            "INFO:tensorflow:loss = 0.15284842, step = 9201 (1.923 sec)\n",
            "INFO:tensorflow:global_step/sec: 51.2257\n",
            "INFO:tensorflow:loss = 0.23262, step = 9301 (1.952 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9371 into /tmp/tmp3ybvbz_d/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.28840464.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.contrib.estimator.python.estimator.dnn.DNNEstimator at 0x7f6aaf3170b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "dMgti0YmJO7F",
        "colab_type": "code",
        "outputId": "90fcc368-2a52-430d-c2c4-5df948d8ad86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "# Define our eval input_fn and run eval\n",
        "eval_input_fn = tf.estimator.inputs.numpy_input_fn({\"descriptions\": np.array(test_descriptions).astype(np.str)}, test_encoded.astype(np.int32), shuffle=False)\n",
        "estimator.evaluate(input_fn=eval_input_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-09-10-15:28:22\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp3ybvbz_d/model.ckpt-9371\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2018-09-10-15:28:33\n",
            "INFO:tensorflow:Saving dict for global step 9371: auc = 0.91783077, auc_precision_recall = 0.749413, average_loss = 0.24026607, global_step = 9371, loss = 0.24114418\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9371: /tmp/tmp3ybvbz_d/model.ckpt-9371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.91783077,\n",
              " 'auc_precision_recall': 0.749413,\n",
              " 'average_loss': 0.24026607,\n",
              " 'global_step': 9371,\n",
              " 'loss': 0.24114418}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "p3vYUGdIHHWA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This model achieved **91.7%** AUC, and** 75%** precision / recall."
      ]
    },
    {
      "metadata": {
        "id": "mcPyCfmWABVO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generating predictions on new data\n",
        "Now for the most fun part! Let's generate predictions on movie descriptions our model hasn't seen before. We'll define an array of 3 new description strings (the comments indicate the correct genres) and create a `predict_input_fn`. Then we'll display the top 2 genres along with their confidence percentages for each of the 3 movies."
      ]
    },
    {
      "metadata": {
        "id": "ixlCKF6NEkTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test our model on some raw description data\n",
        "raw_test = [\n",
        "    \"An examination of our dietary choices and the food we put in our bodies. Based on Jonathan Safran Foer's memoir.\", # Documentary\n",
        "    \"After escaping an attack by what he claims was a 70-foot shark, Jonas Taylor must confront his fears to save those trapped in a sunken submersible.\", # Action, Adventure\n",
        "    \"A teenager tries to survive the last week of her disastrous eighth-grade year before leaving to start high school.\", # Comedy\n",
        "]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XHpMIWFsE4OB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "predict_input_fn = tf.estimator.inputs.numpy_input_fn({\"descriptions\": np.array(raw_test).astype(np.str)}, shuffle=False)\n",
        "results = estimator.predict(predict_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iMVzrHpPDvoy",
        "colab_type": "code",
        "outputId": "34fc1762-8973-4e82-cdae-ca842b9b5d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "# Display predictions\n",
        "for movie_genres in results:\n",
        "  top_2 = movie_genres['probabilities'].argsort()[-2:][::-1]\n",
        "  for genre in top_2:\n",
        "    text_genre = encoder.classes_[genre]\n",
        "    print(text_genre + ': ' + str(round(movie_genres['probabilities'][genre] * 100, 2)) + '%')\n",
        "  print('')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp3ybvbz_d/model.ckpt-9371\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "Documentary: 98.42%\n",
            "Comedy: 15.77%\n",
            "\n",
            "Horror: 88.09%\n",
            "Thriller: 39.53%\n",
            "\n",
            "Comedy: 64.15%\n",
            "Horror: 27.68%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CfZTfK-e7MJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}