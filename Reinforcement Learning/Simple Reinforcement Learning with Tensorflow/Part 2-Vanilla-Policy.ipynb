{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 2-Vanilla-Policy.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:py2]",
      "language": "python",
      "name": "conda-env-py2-py"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "2rMd8XWBWe6Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"60px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"20px\" vspace=\"5px\">\n",
        "\n",
        "# Simple Reinforcement Learning in Tensorflow Part 2-b: \n",
        "## Vanilla Policy Gradient Agent\n",
        "This tutorial contains a simple example of how to build a policy-gradient based agent that can solve the CartPole problem. For more information, see this [Medium post](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.mtwpvfi8b). This implementation is generalizable to more than two actions.\n",
        "\n",
        "For more Reinforcement Learning algorithms, including DQN and Model-based learning in Tensorflow, see my Github repo, [DeepRL-Agents](https://github.com/awjuliani/DeepRL-Agents). "
      ]
    },
    {
      "metadata": {
        "id": "nAN7lFKrWkL1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/Reinforcement%20Learning/Simple%20Reinforcement%20Learning%20with%20Tensorflow/Part%202-Vanilla-Policy.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/Reinforcement%20Learning/Simple%20Reinforcement%20Learning%20with%20Tensorflow/Part%202-Vanilla-Policy.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "2yYosl870svq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57d35b58-c6b5-45ea-f7a7-7d2936847974"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#installing dependencies#instal \n",
        "!apt-get -qq -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!pip -q install gym\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting templates from packages: 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4YllDkldWe6Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    xrange = xrange\n",
        "except:\n",
        "    xrange = range"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LACio0jiWe6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41d4c286-dfe0-41af-a113-5e2f294d86d1"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jv9ke4yXWe61",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Policy-Based Agent"
      ]
    },
    {
      "metadata": {
        "id": "0AO2G4OUWe62",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "\n",
        "def discount_rewards(r):\n",
        "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    discounted_r = np.zeros_like(r)\n",
        "    running_add = 0\n",
        "    for t in reversed(xrange(0, r.size)):\n",
        "        running_add = running_add * gamma + r[t]\n",
        "        discounted_r[t] = running_add\n",
        "    return discounted_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vW62btZnWe7Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class agent():\n",
        "    def __init__(self, lr, s_size,a_size,h_size):\n",
        "        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
        "        self.state_in= tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
        "        hidden = slim.fully_connected(self.state_in,h_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
        "        self.output = slim.fully_connected(hidden,a_size,activation_fn=tf.nn.softmax,biases_initializer=None)\n",
        "        self.chosen_action = tf.argmax(self.output,1)\n",
        "\n",
        "        #The next six lines establish the training procedure. We feed the reward and chosen action into the network\n",
        "        #to compute the loss, and use it to update the network.\n",
        "        self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
        "        self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
        "        \n",
        "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
        "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
        "\n",
        "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
        "        \n",
        "        tvars = tf.trainable_variables()\n",
        "        self.gradient_holders = []\n",
        "        for idx,var in enumerate(tvars):\n",
        "            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
        "            self.gradient_holders.append(placeholder)\n",
        "        \n",
        "        self.gradients = tf.gradients(self.loss,tvars)\n",
        "        \n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yXhmlh77We7g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the Agent"
      ]
    },
    {
      "metadata": {
        "id": "596p277ZWe8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "886d759d-5d0b-4435-a070-88c1e0e93aef"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph() #Clear the Tensorflow graph.\n",
        "\n",
        "myAgent = agent(lr=1e-2,s_size=4,a_size=2,h_size=8) #Load the agent.\n",
        "\n",
        "total_episodes = 5000 #Set total number of episodes to train agent on.\n",
        "max_ep = 999\n",
        "update_frequency = 5\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the tensorflow graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    i = 0\n",
        "    total_reward = []\n",
        "    total_lenght = []\n",
        "        \n",
        "    gradBuffer = sess.run(tf.trainable_variables())\n",
        "    for ix,grad in enumerate(gradBuffer):\n",
        "        gradBuffer[ix] = grad * 0\n",
        "        \n",
        "    while i < total_episodes:\n",
        "        s = env.reset()\n",
        "        running_reward = 0\n",
        "        ep_history = []\n",
        "        for j in range(max_ep):\n",
        "            #Probabilistically pick an action given our network outputs.\n",
        "            a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[s]})\n",
        "            a = np.random.choice(a_dist[0],p=a_dist[0])\n",
        "            a = np.argmax(a_dist == a)\n",
        "\n",
        "            s1,r,d,_ = env.step(a) #Get our reward for taking an action given a bandit.\n",
        "            ep_history.append([s,a,r,s1])\n",
        "            s = s1\n",
        "            running_reward += r\n",
        "            if d == True:\n",
        "                #Update the network.\n",
        "                ep_history = np.array(ep_history)\n",
        "                ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
        "                feed_dict={myAgent.reward_holder:ep_history[:,2],\n",
        "                        myAgent.action_holder:ep_history[:,1],myAgent.state_in:np.vstack(ep_history[:,0])}\n",
        "                grads = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
        "                for idx,grad in enumerate(grads):\n",
        "                    gradBuffer[idx] += grad\n",
        "\n",
        "                if i % update_frequency == 0 and i != 0:\n",
        "                    feed_dict= dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
        "                    _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
        "                    for ix,grad in enumerate(gradBuffer):\n",
        "                        gradBuffer[ix] = grad * 0\n",
        "                \n",
        "                total_reward.append(running_reward)\n",
        "                total_lenght.append(j)\n",
        "                break\n",
        "\n",
        "        \n",
        "            #Update our running tally of scores.\n",
        "        if i % 100 == 0:\n",
        "            print(np.mean(total_reward[-100:]))\n",
        "        i += 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.0\n",
            "36.54\n",
            "46.68\n",
            "63.11\n",
            "91.3\n",
            "131.16\n",
            "127.18\n",
            "149.82\n",
            "172.82\n",
            "183.36\n",
            "189.11\n",
            "194.51\n",
            "190.07\n",
            "181.71\n",
            "194.99\n",
            "198.22\n",
            "199.38\n",
            "198.87\n",
            "197.69\n",
            "196.89\n",
            "198.73\n",
            "197.34\n",
            "197.5\n",
            "196.03\n",
            "192.31\n",
            "170.8\n",
            "167.42\n",
            "152.66\n",
            "170.09\n",
            "181.23\n",
            "197.84\n",
            "185.57\n",
            "191.77\n",
            "195.37\n",
            "197.0\n",
            "189.84\n",
            "175.24\n",
            "184.82\n",
            "187.51\n",
            "176.74\n",
            "142.59\n",
            "146.79\n",
            "176.28\n",
            "175.67\n",
            "176.41\n",
            "192.49\n",
            "198.72\n",
            "199.73\n",
            "199.76\n",
            "199.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LTnvbLG3ejXd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Observation: Intuitively the above allows each action to be a little bit responsible for not only the immediate reward, but all the rewards that followed. We now use this modified reward as an estimation of the advantage in our loss equation.\n",
        "\n",
        "The above observation is the mean total rewards start to increase and then keep oscillating around its optimal. "
      ]
    }
  ]
}