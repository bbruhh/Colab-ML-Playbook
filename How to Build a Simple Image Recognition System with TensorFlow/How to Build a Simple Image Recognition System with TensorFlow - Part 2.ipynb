{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Image Classification Models for the CIFAR-10 dataset using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import data_helpers\n",
    "import two_layer_fc\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters as external flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "batch_size = <absl.flags._flag.Flag object at 0x000002CBF70240B8>\n",
      "f = <absl.flags._flag.Flag object at 0x000002CBF70241D0>\n",
      "hidden1 = <absl.flags._flag.Flag object at 0x000002CBF7027EF0>\n",
      "learning_rate = <absl.flags._flag.Flag object at 0x000002CBF7027EB8>\n",
      "max_steps = <absl.flags._flag.Flag object at 0x000002CBF7027F60>\n",
      "reg_constant = <absl.flags._flag.Flag object at 0x000002CBF7024128>\n",
      "train_dir = <absl.flags._flag.Flag object at 0x000002CBF7024080>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Learning rate for the training.')\n",
    "flags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer('hidden1', 120, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('batch_size', 400,\n",
    "  'Batch size. Must divide dataset sizes without remainder.')\n",
    "flags.DEFINE_string('train_dir', 'tf_logs',\n",
    "  'Directory to put the training data.')\n",
    "flags.DEFINE_float('reg_constant', 0.1, 'Regularization constant.')\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "FLAGS(sys.argv)\n",
    "\n",
    "print('\\nParameters:')\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "  print('{} = {}'.format(attr, value))\n",
    "print()\n",
    "\n",
    "IMAGE_PIXELS = 3072\n",
    "CLASSES = 10\n",
    "\n",
    "beginTime = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put logs for each run in separate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = FLAGS.train_dir + '/' + datetime.now().strftime('%Y%m%d-%H%M%S') + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncommenting these lines removes randomness\n",
    "# You'll get exactly the same result on each run\n",
    "# np.random.seed(1)\n",
    "# tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = data_helpers.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Tensorflow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_placeholder = tf.placeholder(tf.float32, shape=[None, IMAGE_PIXELS],\n",
    "  name='images')\n",
    "labels_placeholder = tf.placeholder(tf.int64, shape=[None], name='image-labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation for the classifier's result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = two_layer_fc.inference(images_placeholder, IMAGE_PIXELS,\n",
    "  FLAGS.hidden1, CLASSES, reg_constant=FLAGS.reg_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation for the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = two_layer_fc.loss(logits, labels_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation for the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = two_layer_fc.training(loss, FLAGS.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation calculating the accuracy of our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = two_layer_fc.evaluation(logits, labels_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation merging summary data for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define saver to save model state at checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, training accuracy 0.13\n",
      "Step 100, training accuracy 0.3125\n",
      "Step 200, training accuracy 0.335\n",
      "Step 300, training accuracy 0.4075\n",
      "Step 400, training accuracy 0.4225\n",
      "Step 500, training accuracy 0.425\n",
      "Step 600, training accuracy 0.4475\n",
      "Step 700, training accuracy 0.495\n",
      "Step 800, training accuracy 0.525\n",
      "Step 900, training accuracy 0.4675\n",
      "Saved checkpoint\n",
      "Step 1000, training accuracy 0.5\n",
      "Step 1100, training accuracy 0.5125\n",
      "Step 1200, training accuracy 0.5175\n",
      "Step 1300, training accuracy 0.465\n",
      "Step 1400, training accuracy 0.51\n",
      "Step 1500, training accuracy 0.5475\n",
      "Step 1600, training accuracy 0.525\n",
      "Step 1700, training accuracy 0.5225\n",
      "Step 1800, training accuracy 0.575\n",
      "Step 1900, training accuracy 0.5275\n",
      "Saved checkpoint\n",
      "Test accuracy 0.4526\n",
      "Total time: 69.60s\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  # Initialize variables and create summary-writer\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "  # Generate input data batches\n",
    "  zipped_data = zip(data_sets['images_train'], data_sets['labels_train'])\n",
    "  batches = data_helpers.gen_batch(list(zipped_data), FLAGS.batch_size,\n",
    "    FLAGS.max_steps)\n",
    "\n",
    "  for i in range(FLAGS.max_steps):\n",
    "\n",
    "    # Get next input data batch\n",
    "    batch = next(batches)\n",
    "    images_batch, labels_batch = zip(*batch)\n",
    "    feed_dict = {\n",
    "      images_placeholder: images_batch,\n",
    "      labels_placeholder: labels_batch\n",
    "    }\n",
    "\n",
    "    # Periodically print out the model's current accuracy\n",
    "    if i % 100 == 0:\n",
    "      train_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n",
    "      print('Step {:d}, training accuracy {:g}'.format(i, train_accuracy))\n",
    "      summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "      summary_writer.add_summary(summary_str, i)\n",
    "\n",
    "    # Perform a single training step\n",
    "    sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "\n",
    "    # Periodically save checkpoint\n",
    "    if (i + 1) % 1000 == 0:\n",
    "      checkpoint_file = os.path.join(FLAGS.train_dir, 'checkpoint')\n",
    "      saver.save(sess, checkpoint_file, global_step=i)\n",
    "      print('Saved checkpoint')\n",
    "\n",
    "  # After finishing the training, evaluate on the test set\n",
    "  test_accuracy = sess.run(accuracy, feed_dict={\n",
    "    images_placeholder: data_sets['images_test'],\n",
    "    labels_placeholder: data_sets['labels_test']})\n",
    "  print('Test accuracy {:g}'.format(test_accuracy))\n",
    "\n",
    "endTime = time.time()\n",
    "print('Total time: {:5.2f}s'.format(endTime - beginTime))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
