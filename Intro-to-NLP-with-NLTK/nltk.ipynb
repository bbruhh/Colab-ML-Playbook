{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to NLP with NLTK",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "9J7p406abzgl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"60px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"20px\" vspace=\"5px\">\n",
        "\n",
        "<h1>Introduction to NLP with NLTK</h1>\n"
      ]
    },
    {
      "metadata": {
        "id": "E5UsQ0QTsqIO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/Intro-to-NLP-with-NLTK/nltk.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/Intro-to-NLP-with-NLTK/nltk.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "9wi5kfGdhK0R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## I. Text Segmentation\n",
        "Text Segmentation is the process of transforming text into meaningful units. These units can be words, sentences or different topics."
      ]
    },
    {
      "metadata": {
        "id": "oYZkU7ZN3CL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "14136e66-361f-4aa2-dcb1-ce98d961e1bd"
      },
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"CODE is founded by Mr. Bachem. Studying at CODE will be unlike any other higher education experience. Our intensive, interdisciplinary bachelor’s programs are designed to dramatically improve the way you work and to prepare you for the reality of tomorrow’s workplace.\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2., 3., 4.],\n",
              "       [5., 6., 7.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "fkJYZgPMuk2W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split it into sentences\n",
        "print(sent_tokenize(text))\n",
        "['CODE is founded by Mr. Bachem.', 'Studying at CODE will be unlike any other higher education experience.', 'Our intensive, interdisciplinary bachelor’s programs are designed to dramatically improve the way you work and to prepare you for the reality of tomorrow’s workplace.']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XgUFHHyEumt-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split into words\n",
        "print(word_tokenize(text))\n",
        "['CODE', 'is', 'founded', 'by', 'Mr.', 'Bachem', '.', 'Studying', 'at', 'CODE', 'will', 'be', 'unlike', 'any', 'other', 'higher', 'education', 'experience', '.', 'Our', 'intensive', ',', 'interdisciplinary', 'bachelor', '’', 's', 'programs', 'are', 'designed', 'to', 'dramatically', 'improve', 'the', 'way', 'you', 'work', 'and', 'to', 'prepare', 'you', 'for', 'the', 'reality', 'of', 'tomorrow', '’', 's', 'workplace', '.']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j1qSUmAXupaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"beneath the extraodrinary staircase...\"\n",
        "\n",
        "tokenize= sent_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VDz1L3hiut9u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## II. Stop Words & Word Segmentation\n",
        "Also part of Natural Language are words that are basically useless, which are referred to as \"stop words\". Since we dont want that these words extend our processing time or take up unnecessary space in our database, we will remove them."
      ]
    },
    {
      "metadata": {
        "id": "TqWJV9o2u0D2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Removing stop words from text\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"\"\"CODE is founded by Mr. Bachem. Studying at CODE will be unlike \n",
        "any other higher education experience. Our intensive, interdisciplinary \n",
        "bachelor’s programs are designed to dramatically improve the way you work \n",
        "and to prepare you for the reality of tomorrow’s workplace.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CMN1EGVZu3xn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# defining the stop words we will use\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokens = word_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fI13uauHu58H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# filter the text for stop words\n",
        "filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rtoq6w2-u73P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# show just the tokenized text\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dqhOp6GMu9v-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# show filtered tokenized text\n",
        "print(filtered_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7yyq8nrqvHRs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## III. Stemming"
      ]
    },
    {
      "metadata": {
        "id": "G7K64g_3vKZC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LL1cXBEOvRKU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stemming single words:"
      ]
    },
    {
      "metadata": {
        "id": "P8f5yEGTvTeT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "example_wordsexample_  = [\"ride\",\"riding\", \"rider\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HSR-AFQWvU_0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for w in example_words:\n",
        "    print(ps.stem(w))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OHcwldbtvZIc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stemming sentences:"
      ]
    },
    {
      "metadata": {
        "id": "0FAUrdJFvcSb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_text = \"\"\"CODE is a newly founded private university of applied sciences that is embedded into the vibrant \n",
        "network of Berlin's digital economy.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7JFzF5Jvd01",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words = word_tokenize(new_text)\n",
        "\n",
        "for w in words:\n",
        "    print(ps.stem(w))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rUyVJSvIQxyT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## IV. Parsing (Speech Tagging & Chunking)"
      ]
    },
    {
      "metadata": {
        "id": "CyHtE6HLQ1EJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Speech Tagging\n",
        "Speech Tagging in NLTK is the process of labeling words in a sentence as nouns, adjectives, verbs and more.\n",
        "\n",
        "Fortunately, NLTK provides us with a sentence tokenizer called the \"PunktSentenceTokenizer\", which is a un-supervised ML algorithm that can be trained on any text corpus you wish to."
      ]
    },
    {
      "metadata": {
        "id": "lWbolayeQ7mP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import  nltk\n",
        " from nltk.tokenize import PunktSentenceTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4r_HTHHGRJic",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# using novels by chesterton\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import gutenberg\n",
        "test = gutenberg.raw(\"chesterton-ball.txt\")\n",
        "train = gutenberg.raw(\"chesterton-brown.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BEzUmHMgRMZz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train tokenizer\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train)\n",
        "# tokenize chesterton ball\n",
        "tokenized = custom_sent_tokenizer.tokenize(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y1mC8Bt8ROR7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tag_text():\n",
        "    try:\n",
        "        for i in tokenized[:7]:\n",
        "            actual_words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(actual_words)\n",
        "            print(tagged)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "tag_text()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IseyJgGxRW7_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Chunking Text\n",
        "Chunking is the process of grouping words into more meaningful junks than just the speech tags. This can be things such as \"noun phrases\" or \"verb phrases\". With chunking you can get a parse tree.\n",
        "\n",
        "We will search for chunks that correspond to individual noun phrases."
      ]
    },
    {
      "metadata": {
        "id": "cr3h78KIRY-4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# using pre-tagged text out of simplicity\n",
        "text = [(\"the\", \"DT\"), (\"huge\", \"JJ\"), (\"german\", \"JJ\"), (\"Rottweiler\", \"NN\"), \n",
        "        (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "96VW0c_PRdFH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define a noun-phrase as:\n",
        "# np = determiner + adjective + singular noun\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
        "\n",
        "# apply grammar to regexparser\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "\n",
        "# do the chunking\n",
        "result = cp.parse(text) \n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zYi3bIORhBx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## V. Sentiment Analysis using Keras\n",
        "\n",
        "Blogpost: https://towardsdatascience.com/how-to-build-a-neural-network-with-keras-e8faa33d0ae4"
      ]
    },
    {
      "metadata": {
        "id": "bchwewQyRm4V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.datasets import imdb\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
        "def vectorize(sequences, dimension = 10000):\n",
        " results = np.zeros((len(sequences), dimension))\n",
        " for i, sequence in enumerate(sequences):\n",
        "  results[i, sequence] = 1\n",
        " return results\n",
        " \n",
        "data = vectorize(data)\n",
        "targets = np.array(targets).astype(\"float32\")\n",
        "test_x = data[:10000]\n",
        "test_y = targets[:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = targets[10000:]\n",
        "model = models.Sequential()\n",
        "# Input - Layer\n",
        "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
        "# Hidden - Layers\n",
        "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "# Output- Layer\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "model.summary()\n",
        "# compiling the model\n",
        "model.compile(\n",
        " optimizer = \"adam\",\n",
        " loss = \"binary_crossentropy\",\n",
        " metrics = [\"accuracy\"]\n",
        ")\n",
        "results = model.fit(\n",
        " train_x, train_y,\n",
        " epochs= 2,\n",
        " batch_size = 500,\n",
        " validation_data = (test_x, test_y)\n",
        ")\n",
        "print(\"Test-Accuracy:\", np.mean(results.history[\"val_acc\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d0QgV3aIRuE1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Example of a IMDB Review:"
      ]
    },
    {
      "metadata": {
        "id": "Wifj6xemRwYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qf-GT3h_Ry1l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index = imdb.get_word_index()\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] )\n",
        "print(decoded)\n",
        "print(\"Label:\", targets[0], \", which means positive.\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}