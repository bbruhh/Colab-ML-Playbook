{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to Use The Pre-Trained VGG Model to Classify Objects in Photographs.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ISR4MnO-kQzR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"60px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"20px\" vspace=\"5px\">\n",
        "# How to Use The Pre-Trained VGG Model to Classify Objects in Photographs\n",
        "\n",
        "Convolutional neural networks are now capable of outperforming humans on some computer vision tasks, such as classifying images.\n",
        "\n",
        "That is, given a photograph of an object, answer the question as to which of 1,000 specific objects the photograph shows.\n",
        "\n",
        "A competition-winning model for this task is the VGG model by researchers at Oxford. What is important about this model, besides its capability of classifying objects in photographs, is that the model weights are freely available and can be loaded and used in your own models and applications.\n",
        "\n",
        "In this tutorial, you will discover the VGG convolutional neural network models for image classification.\n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "\n",
        "About the ImageNet dataset and competition and the VGG winning models.\n",
        "How to load the VGG model in Keras and summarize its structure.\n",
        "How to use the loaded VGG model to classifying objects in ad hoc photographs."
      ]
    },
    {
      "metadata": {
        "id": "6Utg6v0Hkamk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Reference: https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/"
      ]
    },
    {
      "metadata": {
        "id": "xZD_i1QAknz7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/Convolutional%20Neural%20Network/How%20to%20Use%20The%20Pre-Trained%20VGG%20Model%20to%20Classify%20Objects%20in%20Photographs.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/Convolutional%20Neural%20Network/How%20to%20Use%20The%20Pre-Trained%20VGG%20Model%20to%20Classify%20Objects%20in%20Photographs.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "pPoGh6I4p0XS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the VGG Model in Keras"
      ]
    },
    {
      "metadata": {
        "id": "Xi1Y0elkkQzv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "model = VGG16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZQ8a9UVpZTi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "model = VGG16()\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mnPZTuvnph26",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.utils.vis_utils import plot_model\n",
        "model = VGG16()\n",
        "plot_model(model, to_file='vgg.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3GL1iyTep8DK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Develop a Simple Photo Classifier"
      ]
    },
    {
      "metadata": {
        "id": "b8vt7fLYsA31",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Upload a sample photo\n",
        "\n",
        "# Source is from: https://www.flickr.com/photos/jfanaian/4994221690/\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGESplQdqCIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the VGG Model\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "# load the model\n",
        "model = VGG16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uT5sq1mcqj8h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load and Prepare Image\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "# load an image from file\n",
        "image = load_img('mug.jpg', target_size=(224, 224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZwCdNCY4q0TR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "# convert the image pixels to a numpy array\n",
        "image = img_to_array(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YlMvbSDRq-36",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The network expects one or more images as input; that means the input array will need to be 4-dimensional: samples, rows, columns, and channels.\n",
        "\n",
        "We only have one sample (one image). We can reshape the array by calling reshape() and adding the extra dimension."
      ]
    },
    {
      "metadata": {
        "id": "sh4GeNlxq7OZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reshape data for the model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FQrP4PcirIQL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, the image pixels need to be prepared in the same way as the ImageNet training data was prepared. Specifically, from the paper:\n",
        "\n",
        "\"The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel.\"\n",
        "\n",
        "Keras provides a function called preprocess_input() to prepare new input for the network."
      ]
    },
    {
      "metadata": {
        "id": "ZJ2O7vmErI46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import preprocess_input\n",
        "# prepare the image for the VGG model\n",
        "image = preprocess_input(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9nJTEo6DrbBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make a Prediction\n",
        "\n",
        "# predict the probability across all output classes\n",
        "yhat = model.predict(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ac0onbp0rm-3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Keras provides a function to interpret the probabilities called decode_predictions().\n",
        "\n",
        "It can return a list of classes and their probabilities in case you would like to present the top 3 objects that may be in the photo."
      ]
    },
    {
      "metadata": {
        "id": "3zP6jJd8rgyJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Interpret Prediction\n",
        "\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "# convert the probabilities to class labels\n",
        "label = decode_predictions(yhat)\n",
        "# retrieve the most likely result, e.g. highest probability\n",
        "label = label[0][0]\n",
        "# print the classification\n",
        "print('%s (%.2f%%)' % (label[1], label[2]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}