{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to Use The Pre-Trained VGG Model to Classify Objects in Photographs.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ISR4MnO-kQzR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"60px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"20px\" vspace=\"5px\">\n",
        "# How to Use The Pre-Trained VGG Model to Classify Objects in Photographs\n",
        "\n",
        "Convolutional neural networks are now capable of outperforming humans on some computer vision tasks, such as classifying images.\n",
        "\n",
        "That is, given a photograph of an object, answer the question as to which of 1,000 specific objects the photograph shows.\n",
        "\n",
        "A competition-winning model for this task is the VGG model by researchers at Oxford. What is important about this model, besides its capability of classifying objects in photographs, is that the model weights are freely available and can be loaded and used in your own models and applications.\n",
        "\n",
        "In this tutorial, you will discover the VGG convolutional neural network models for image classification.\n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "\n",
        "About the ImageNet dataset and competition and the VGG winning models.\n",
        "How to load the VGG model in Keras and summarize its structure.\n",
        "How to use the loaded VGG model to classifying objects in ad hoc photographs."
      ]
    },
    {
      "metadata": {
        "id": "6Utg6v0Hkamk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Reference: https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/"
      ]
    },
    {
      "metadata": {
        "id": "xZD_i1QAknz7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/Convolutional%20Neural%20Network/How%20to%20Use%20The%20Pre-Trained%20VGG%20Model%20to%20Classify%20Objects%20in%20Photographs.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/Convolutional%20Neural%20Network/How%20to%20Use%20The%20Pre-Trained%20VGG%20Model%20to%20Classify%20Objects%20in%20Photographs.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "hSnuVWxQkQzd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import the dataset. Here, we'll need to convert the labels to a one-hot encoding, and we'll reshape the MNIST images to (784,)."
      ]
    },
    {
      "metadata": {
        "id": "Xi1Y0elkkQzv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_input = tf.estimator.inputs.numpy_input_fn(\n",
        "    {'x': x_train},\n",
        "    y_train, \n",
        "    num_epochs=None, # repeat forever\n",
        "    shuffle=True # \n",
        ")\n",
        "\n",
        "test_input = tf.estimator.inputs.numpy_input_fn(\n",
        "    {'x': x_test},\n",
        "    y_test,\n",
        "    num_epochs=1, # loop through the dataset once\n",
        "    shuffle=False # don't shuffle the test data\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}