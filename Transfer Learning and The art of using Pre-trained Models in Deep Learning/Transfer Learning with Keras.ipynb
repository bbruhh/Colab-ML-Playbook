{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network with Keras.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "L0dT79EZSDtB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"60px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"20px\" vspace=\"5px\">\n",
        "\n",
        "# Transfer learning & The art of using Pre-trained Models in Deep Learning"
      ]
    },
    {
      "metadata": {
        "id": "RgW8s3sM7rDw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* References:\n",
        "\n",
        "[https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/](https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/)"
      ]
    },
    {
      "metadata": {
        "id": "j4gHxXEaSN4g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/learning-stack/Colab-ML-Playbook/blob/master/Transfer%20Learning%20and%20The%20art%20of%20using%20Pre-trained%20Models%20in%20Deep%20Learning/Transfer%20Learning%20with%20Keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/learning-stack/Colab-ML-Playbook/blob/master/Transfer%20Learning%20and%20The%20art%20of%20using%20Pre-trained%20Models%20in%20Deep%20Learning/Transfer%20Learning%20with%20Keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "pGEasn5XaGrH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "\n",
        "from keras.models import Sequential\n",
        "from scipy.misc import imread\n",
        "get_ipython().magic('matplotlib inline')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "import pandas as pd\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import numpy as np\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "\n",
        "from scipy.misc import imresize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mfCDtumlbhSZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "import io, gzip, requests, gc\n",
        "\n",
        "train_image_url = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\"\n",
        "train_label_url = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\"\n",
        "test_image_url = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\"\n",
        "test_label_url = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\"\n",
        "\n",
        "\n",
        "def readRemoteGZipFile(url, isLabel=True):\n",
        "    response=requests.get(url, stream=True)\n",
        "    gzip_content = response.content\n",
        "    fObj = io.BytesIO(gzip_content)\n",
        "    content = gzip.GzipFile(fileobj=fObj).read()\n",
        "    if isLabel:\n",
        "        offset=8\n",
        "    else:\n",
        "        offset=16\n",
        "    result = np.frombuffer(content, dtype=np.uint8, offset=offset)    \n",
        "    return(result)\n",
        "\n",
        "train_labels = readRemoteGZipFile(train_label_url, isLabel=True)\n",
        "train_images_raw = readRemoteGZipFile(train_image_url, isLabel=False)\n",
        "test_labels = readRemoteGZipFile(test_label_url, isLabel=True)\n",
        "test_images_raw = readRemoteGZipFile(test_image_url, isLabel=False)\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lhTSGmPaJCK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "73071cea-f213-4461-9fd4-a347971d5cd4"
      },
      "cell_type": "code",
      "source": [
        "#download and set mnist data file\n",
        "!wget -P R \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\"\n",
        "!wget -P R \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\"\n",
        "!wget -P R \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\"\n",
        "!wget -P R \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"\n",
        "\n",
        "!gunzip  R/train-images-idx3-ubyte.gz\n",
        "!gunzip  R/train-labels-idx1-ubyte.gz\n",
        "!gunzip  R/t10k-images-idx3-ubyte.gz\n",
        "!gunzip  R/t10k-labels-idx1-ubyte.gz\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log’.\n",
            "\n",
            "Redirecting output to ‘wget-log.1’.\n",
            "\n",
            "Redirecting output to ‘wget-log.2’.\n",
            "\n",
            "Redirecting output to ‘wget-log.3’.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "acZbwB6Ws9LF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "703d8318-8a78-466c-de03-8e17de99a9c0"
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import gzip\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print(\"Downloading \" + file_name + \" ... \")\n",
        "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    print(\"Done\")\n",
        "\n",
        "def download_mnist():\n",
        "    for v in key_file.values():\n",
        "       _download(v)\n",
        "\n",
        "def _load_label(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "\n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    print(\"Done\")\n",
        "\n",
        "    return labels\n",
        "\n",
        "def _load_img(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "\n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    data = data.reshape(-1, img_size)\n",
        "    print(\"Done\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def _convert_numpy():\n",
        "    dataset = {}\n",
        "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
        "    dataset['train_label'] = _load_label(key_file['train_label'])\n",
        "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
        "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def init_mnist():\n",
        "    download_mnist()\n",
        "    dataset = _convert_numpy()\n",
        "    print(\"Creating pickle file ...\")\n",
        "    with open(save_file, 'wb') as f:\n",
        "        pickle.dump(dataset, f, -1)\n",
        "    print(\"Done\")\n",
        "\n",
        "def _change_ont_hot_label(X):\n",
        "    T = np.zeros((X.size, 10))\n",
        "    for idx, row in enumerate(T):\n",
        "        row[X[idx]] = 1\n",
        "\n",
        "    return T\n",
        "\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    normalize : Normalize the pixel values\n",
        "    flatten : Flatten the images as one array\n",
        "    one_hot_label : Encode the labels as a one-hot array\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (Trainig Image, Training Label), (Test Image, Test Label)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(save_file):\n",
        "        init_mnist()\n",
        "\n",
        "    with open(save_file, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "\n",
        "    if normalize:\n",
        "        for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].astype(np.float32)\n",
        "            dataset[key] /= 255.0\n",
        "\n",
        "    if not flatten:\n",
        "         for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "    if one_hot_label:\n",
        "        dataset['train_label'] = _change_ont_hot_label(dataset['train_label'])\n",
        "        dataset['test_label'] = _change_ont_hot_label(dataset['test_label'])\n",
        "\n",
        "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])\n",
        "\n",
        "def img_show(img):\n",
        "    pil_img = Image.fromarray(np.uint8(img))\n",
        "    pil_img.show()\n",
        "\n",
        "\n",
        "# Load the MNIST dataset\n",
        "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
        "key_file = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "#dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "dataset_dir = \"R\"\n",
        "save_file = dataset_dir + \"/mnist.pkl\"\n",
        "\n",
        "train_num = 60000\n",
        "test_num = 10000\n",
        "img_dim = (1, 28, 28)\n",
        "img_size = 784\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=False, flatten=True)\n",
        "\n",
        "# Show the sample image\n",
        "img = x_train[0]\n",
        "label = t_train[0]\n",
        "print(label)\n",
        "\n",
        "print(img.shape)\n",
        "img = img.reshape(28, 28)\n",
        "print(img.shape)\n",
        "\n",
        "img_show(img)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done\n",
            "5\n",
            "(784,)\n",
            "(28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f7h9EEtevAe9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_show(img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HSgolOVwpDKu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_img = train_images_raw.reshape(len(train_labels), 224)\n",
        "test_img = test_images_raw.reshape(len(test_labels), 224)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ent0nkSzHOFo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#download and set mnist data file\n",
        "#!wget -P R/Data/Train/ http://www.pjreddie.com/media/files/mnist_train.csv\n",
        "#!wget -P R/Data http://www.pjreddie.com/media/files/mnist_test.csv\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TIQftUp5QYRd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UWarwJL_-5wi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preparing the train dataset\n",
        "\n",
        "#train_img=[]\n",
        "#for i in range(len(train)):\n",
        "#\n",
        "#    temp_img=image.load_img(train_path+train['filename'][i],target_size=(224,224))\n",
        "#\n",
        "#    temp_img=image.img_to_array(temp_img)\n",
        "#\n",
        "#    train_img.append(temp_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-n2BcNH-9x6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#converting train images to array and applying mean subtraction processing\n",
        "\n",
        "train_img=np.array(train_img) \n",
        "train_img=preprocess_input(train_img.astype(float))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DuUZvlSTEMcQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# applying the same procedure with the test dataset\n",
        "#\n",
        "#test_img=[]\n",
        "#for i in range(len(test)):\n",
        "#\n",
        "#    temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,224))\n",
        "#\n",
        "#    temp_img=image.img_to_array(temp_img)\n",
        "#\n",
        "#    test_img.append(temp_img)\n",
        "#\n",
        "test_img=np.array(test_img) \n",
        "test_img=preprocess_input(test_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IvnyEVi1_Afa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loading VGG16 model weights\n",
        "\n",
        "model = VGG16(weights='imagenet', include_top=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iD8Nc4QP_CZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
        "from keras.layers import Input\n",
        "input = Input(shape=(3,200,200),name = 'train_img')\n",
        "features_train=model.predict(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VFW1nGz_EdJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
        "\n",
        "features_test=model.predict(test_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CM2xvYLJ_HUC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# flattening the layers to conform to MLP input\n",
        "\n",
        "train_x=features_train.reshape(49000,25088)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FULR_tDO_Olh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# converting target variable to array\n",
        "\n",
        "train_y=np.asarray(train['label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eFgL2O9Y_QFp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# performing one-hot encoding for the target variable\n",
        "\n",
        "train_y=pd.get_dummies(train_y)\n",
        "train_y=np.array(train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmlDUIy__SEp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# creating training and validation set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, Y_train, Y_valid=train_test_split(train_x,train_y,test_size=0.3, random_state=42)\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O6ODEgiV_U9a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# creating a mlp model\n",
        "\n",
        "from keras.layers import Dense, Activation\n",
        "model=Sequential()\n",
        "\n",
        "model.add(Dense(1000, input_dim=25088, activation='relu',kernel_initializer='uniform'))\n",
        "keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)\n",
        "\n",
        "model.add(Dense(500,input_dim=1000,activation='sigmoid'))\n",
        "keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)\n",
        "\n",
        "model.add(Dense(150,input_dim=500,activation='sigmoid'))\n",
        "keras.layers.core.Dropout(0.2, noise_shape=None, seed=None)\n",
        "\n",
        "model.add(Dense(units=10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wyMQyObB_Wph",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fitting the model \n",
        "\n",
        "model.fit(X_train, Y_train, epochs=20, batch_size=128,validation_data=(X_valid,Y_valid))\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}